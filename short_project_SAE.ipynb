{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A study of SAE features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem description: indirect object identification\n",
    "\n",
    "- Writing functions for attention SAEs to see what source token contributes to the activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Directly from the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Literal, TypeAlias\n",
    "\n",
    "import circuitsvis as cv\n",
    "import einops\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import requests\n",
    "import torch as t\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from IPython.display import HTML, IFrame, clear_output, display\n",
    "from jaxtyping import Float, Int\n",
    "from openai import OpenAI\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from sae_lens import (\n",
    "    SAE,\n",
    "    ActivationsStore,\n",
    "    HookedSAETransformer,\n",
    "    LanguageModelSAERunnerConfig,\n",
    "    SAEConfig,\n",
    "    SAETrainingRunner,\n",
    "    upload_saes_to_huggingface,\n",
    ")\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "# from sae_vis import SaeVisConfig, SaeVisData, SaeVisLayoutConfig\n",
    "from tabulate import tabulate\n",
    "from torch import Tensor, nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import ActivationCache, HookedTransformer\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens.utils import get_act_name, test_prompt, to_numpy\n",
    "\n",
    "device = t.device(\"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# There's a single utils & tests file for both parts 3.1 & 3.2\n",
    "# import part31_superposition_and_saes.utils as utils\n",
    "# from plotly_utils import imshow, line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def s(tensor):\n",
    "    \"\"\"\n",
    "    Simple helper function to print the shape of a tensor.\n",
    "    \n",
    "    Args:\n",
    "        tensor: A PyTorch tensor or any object with a .shape attribute\n",
    "    \n",
    "    Example:\n",
    "        attnout = torch.randn(32, 768)\n",
    "        s(attnout)  # Output: shape of attnout is torch.Size([32, 768])\n",
    "    \"\"\"\n",
    "    # Get the name of the variable from the caller's frame\n",
    "    frame = inspect.currentframe().f_back\n",
    "    calling_line = inspect.getframeinfo(frame).code_context[0].strip()\n",
    "    # Extract variable name from the function call\n",
    "    # This looks for s(variable_name) pattern\n",
    "    import re\n",
    "    match = re.search(r's\\((.*?)\\)', calling_line)\n",
    "    if match:\n",
    "        var_name = match.group(1).strip()\n",
    "    else:\n",
    "        var_name = \"tensor\"\n",
    "        \n",
    "    if hasattr(tensor, 'shape'):\n",
    "        print(f\"Shape of [{var_name}]: {tensor.shape}\")\n",
    "    else:\n",
    "        print(f\"{var_name} has no shape attribute. Type: {type(tensor)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the language model\n",
    "\n",
    "We loaded GPT-2-small and Gemma-2.2b, and their corresponding trained SAEs. Specifically we are using the SAE release\n",
    "\n",
    "```python\n",
    "gemmascope_sae_release = \"gemma-scope-2b-pt-res-canonical\"\n",
    "gemmascope_sae_id = \"layer_20/width_16k/canonical\"\n",
    "gemma_2_2b_sae = SAE.from_pretrained(gemmascope_sae_release, gemmascope_sae_id, device=str(device))[0]\n",
    "```\n",
    "\n",
    "Also expand to see detailed architectural info on `gemma_2_2b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15896\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sae_lens\\sae.py:146: UserWarning:\n",
      "\n",
      "\n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t.set_grad_enabled(False)\n",
    "\n",
    "gpt2: HookedSAETransformer = HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "gpt2_sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb\",\n",
    "    sae_id=\"blocks.7.hook_resid_pre\",\n",
    "    device=str(device),\n",
    ")\n",
    "\n",
    "# print(tabulate(gpt2_sae.cfg.__dict__.items(), headers=[\"name\", \"value\"], tablefmt=\"simple_outline\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "str expected, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m HUGGINGFACE_KEY \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHUGGINGFACE_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m HUGGINGFACE_KEY\n\u001b[0;32m      6\u001b[0m USING_GEMMA \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHUGGINGFACE_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m USING_GEMMA:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\os.py:684\u001b[0m, in \u001b[0;36m_Environ.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value):\n\u001b[0;32m    683\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)\n\u001b[1;32m--> 684\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencodevalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    685\u001b[0m     putenv(key, value)\n\u001b[0;32m    686\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[key] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\os.py:742\u001b[0m, in \u001b[0;36m_createenviron.<locals>.check_str\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    740\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_str\u001b[39m(value):\n\u001b[0;32m    741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 742\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr expected, not \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[1;31mTypeError\u001b[0m: str expected, not int"
     ]
    }
   ],
   "source": [
    "## the task of arithmetic does not work well for gpt2_small. Switching to gemma\n",
    "HUGGINGFACE_KEY = 0\n",
    "import os\n",
    "os.environ[\"HUGGINGFACE_KEY\"] = HUGGINGFACE_KEY\n",
    "\n",
    "USING_GEMMA = os.environ.get(\"HUGGINGFACE_KEY\") is not None\n",
    "\n",
    "if USING_GEMMA:\n",
    "    !huggingface-cli login --token $HUGGINGFACE_KEY\n",
    "    gemma_2_2b = HookedSAETransformer.from_pretrained(\"gemma-2-2b\", device=device)\n",
    "\n",
    "    gemmascope_sae_release = \"gemma-scope-2b-pt-res-canonical\"\n",
    "    gemmascope_sae_id = \"layer_20/width_16k/canonical\"\n",
    "    gemma_2_2b_sae = SAE.from_pretrained(gemmascope_sae_release, gemmascope_sae_id, device=str(device))[0]\n",
    "else:\n",
    "    print(\"Please supply your Hugging Face API key before running this cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────────────────────────────┬─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ name                               │ value                                                                                                                                                                                                                                                                                                                                                                                                           │\n",
      "├────────────────────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ n_layers                           │ 26                                                                                                                                                                                                                                                                                                                                                                                                              │\n",
      "│ d_model                            │ 2304                                                                                                                                                                                                                                                                                                                                                                                                            │\n",
      "│ n_ctx                              │ 8192                                                                                                                                                                                                                                                                                                                                                                                                            │\n",
      "│ d_head                             │ 256                                                                                                                                                                                                                                                                                                                                                                                                             │\n",
      "│ model_name                         │ gemma-2-2b                                                                                                                                                                                                                                                                                                                                                                                                      │\n",
      "│ n_heads                            │ 8                                                                                                                                                                                                                                                                                                                                                                                                               │\n",
      "│ d_mlp                              │ 9216                                                                                                                                                                                                                                                                                                                                                                                                            │\n",
      "│ act_fn                             │ gelu_pytorch_tanh                                                                                                                                                                                                                                                                                                                                                                                               │\n",
      "│ d_vocab                            │ 256000                                                                                                                                                                                                                                                                                                                                                                                                          │\n",
      "│ eps                                │ 1e-06                                                                                                                                                                                                                                                                                                                                                                                                           │\n",
      "│ use_attn_result                    │ False                                                                                                                                                                                                                                                                                                                                                                                                           │\n",
      "│ use_attn_scale                     │ True                                                                                                                                                                                                                                                                                                                                                                                                            │\n",
      "│ attn_scale                         │ 16.0                                                                                                                                                                                                                                                                                                                                                                                                            │\n",
      "│ use_split_qkv_input                │ False                                                                                                                                                                                                                                                                                                                                                                                                           │\n",
      "│ use_hook_mlp_in                    │ False                                                                                                                                                                                                                                                                                                                                                                                                           │\n",
      "│ use_attn_in                        │ False                                                                                                                                                                                                                                                                                                                                                                                                           │\n",
      "│ use_local_attn                     │ True                                                                                                                                                                                                                                                                                                                                                                                                            │\n",
      "│ ungroup_grouped_query_attention    │ False                                                                                                                                                                                                                                                                                                                                                                                                           │\n",
      "│ original_architecture              │ Gemma2ForCausalLM                                                                                                                                                                                                                                                                                                                                                                                               │\n",
      "│ from_checkpoint                    │ False                                                                                                                                                                                                                                                                                                                                                                                                           │\n",
      "│ checkpoint_index                   │                                                                                                                                                                                                                                                                                                                                                                                                                 │\n",
      "│ checkpoint_label_type              │                                                                                                                                                                                                                                                                                                                                                                                                                 │\n",
      "│ checkpoint_value                   │                                                                                                                                                                                                                                                                                                                                                                                                                 │\n",
      "│ tokenizer_name                     │ google/gemma-2-2b                                                                                                                                                                                                                                                                                                                                                                                               │\n",
      "│ window_size                        │ 4096                                                                                                                                                                                                                                                                                                                                                                                                            │\n",
      "│ attn_types                         │ ['global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local'] │\n",
      "│ init_mode                          │ gpt2                                                                                                                                                                                                                                                                                                                                                                                                            │\n",
      "│ normalization_type                 │ RMSPre                                                                                                                                                                                                                                                                                                                                                                                                          │\n",
      "│ device                             │ cpu                                                                                                                                                                                                                                                                                                                                                                                                             │\n",
      "│ n_devices                          │ 1                                                                                                                                                                                                                                                                                                                                                                                                               │\n",
      "│ attention_dir                      │ causal                                                                                                                                                                                                                                                                                                                                                                                                          │\n",
      "│ attn_only                          │ False                                                                                                                                                                                                                                                                                                                                                                                                           │\n",
      "│ seed                               │                                                                                                                                                                                                                                                                                                                                                                                                                 │\n",
      "│ initializer_range                  │ 0.02                                                                                                                                                                                                                                                                                                                                                                                                            │\n",
      "│ init_weights                       │ False                                                                                                                                                                                                                                                                                                                                                                                                           │\n",
      "│ scale_attn_by_inverse_layer_idx    │ False                                                                                                                                                                                                                                                                                                                                                                                                           │\n",
      "│ positional_embedding_type          │ rotary                                                                                                                                                                                                                                                                                                                                                                                                          │\n",
      "│ final_rms                          │ True                                                                                                                                                                                                                                                                                                                                                                                                            │\n",
      "│ d_vocab_out                        │ 256000                                                                                                                                                                                                                                                                                                                                                                                                          │\n",
      "│ parallel_attn_mlp                  │ False                                                                                                                                                                                                                                                                                                                                                                                                           │\n",
      "│ rotary_dim                         │ 256                                                                                                                                                                                                                                                                                                                                                                                                             │\n",
      "│ n_params                           │ 2146959360                                                                                                                                                                                                                                                                                                                                                                                                      │\n",
      "│ use_hook_tokens                    │ False                                                                                                                                                                                                                                                                                                                                                                                                           │\n",
      "│ gated_mlp                          │ True                                                                                                                                                                                                                                                                                                                                                                                                            │\n",
      "│ default_prepend_bos                │ True                                                                                                                                                                                                                                                                                                                                                                                                            │\n",
      "│ dtype                              │ torch.float32                                                                                                                                                                                                                                                                                                                                                                                                   │\n",
      "│ tokenizer_prepends_bos             │ True                                                                                                                                                                                                                                                                                                                                                                                                            │\n",
      "│ n_key_value_heads                  │ 4                                                                                                                                                                                                                                                                                                                                                                                                               │\n",
      "│ post_embedding_ln                  │ False                                                                                                                                                                                                                                                                                                                                                                                                           │\n",
      "│ rotary_base                        │ 10000.0                                                                                                                                                                                                                                                                                                                                                                                                         │\n",
      "│ trust_remote_code                  │ False                                                                                                                                                                                                                                                                                                                                                                                                           │\n",
      "│ rotary_adjacent_pairs              │ False                                                                                                                                                                                                                                                                                                                                                                                                           │\n",
      "│ load_in_4bit                       │ False                                                                                                                                                                                                                                                                                                                                                                                                           │\n",
      "│ num_experts                        │                                                                                                                                                                                                                                                                                                                                                                                                                 │\n",
      "│ experts_per_token                  │                                                                                                                                                                                                                                                                                                                                                                                                                 │\n",
      "│ relative_attention_max_distance    │                                                                                                                                                                                                                                                                                                                                                                                                                 │\n",
      "│ relative_attention_num_buckets     │                                                                                                                                                                                                                                                                                                                                                                                                                 │\n",
      "│ decoder_start_token_id             │                                                                                                                                                                                                                                                                                                                                                                                                                 │\n",
      "│ tie_word_embeddings                │ False                                                                                                                                                                                                                                                                                                                                                                                                           │\n",
      "│ use_normalization_before_and_after │ True                                                                                                                                                                                                                                                                                                                                                                                                            │\n",
      "│ attn_scores_soft_cap               │ 50.0                                                                                                                                                                                                                                                                                                                                                                                                            │\n",
      "│ output_logits_soft_cap             │ 30.0                                                                                                                                                                                                                                                                                                                                                                                                            │\n",
      "│ use_NTK_by_parts_rope              │ False                                                                                                                                                                                                                                                                                                                                                                                                           │\n",
      "│ NTK_by_parts_low_freq_factor       │ 1.0                                                                                                                                                                                                                                                                                                                                                                                                             │\n",
      "│ NTK_by_parts_high_freq_factor      │ 4.0                                                                                                                                                                                                                                                                                                                                                                                                             │\n",
      "│ NTK_by_parts_factor                │ 8.0                                                                                                                                                                                                                                                                                                                                                                                                             │\n",
      "└────────────────────────────────────┴─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "## Try loading SAEs at other layers as well?\n",
    "print(tabulate(gemma_2_2b.cfg.__dict__.items(), headers=[\"name\", \"value\"], tablefmt=\"simple_outline\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────────────────────┬──────────────────────────────────┐\n",
      "│ name                         │ value                            │\n",
      "├──────────────────────────────┼──────────────────────────────────┤\n",
      "│ architecture                 │ jumprelu                         │\n",
      "│ d_in                         │ 2304                             │\n",
      "│ d_sae                        │ 16384                            │\n",
      "│ activation_fn_str            │ relu                             │\n",
      "│ apply_b_dec_to_input         │ False                            │\n",
      "│ finetuning_scaling_factor    │ False                            │\n",
      "│ context_size                 │ 1024                             │\n",
      "│ model_name                   │ gemma-2-2b                       │\n",
      "│ hook_name                    │ blocks.20.hook_resid_post        │\n",
      "│ hook_layer                   │ 20                               │\n",
      "│ hook_head_index              │                                  │\n",
      "│ prepend_bos                  │ True                             │\n",
      "│ dataset_path                 │ monology/pile-uncopyrighted      │\n",
      "│ dataset_trust_remote_code    │ True                             │\n",
      "│ normalize_activations        │                                  │\n",
      "│ dtype                        │ float32                          │\n",
      "│ device                       │ cpu                              │\n",
      "│ sae_lens_training_version    │                                  │\n",
      "│ activation_fn_kwargs         │ {}                               │\n",
      "│ neuronpedia_id               │ gemma-2-2b/20-gemmascope-res-16k │\n",
      "│ model_from_pretrained_kwargs │ {}                               │\n",
      "│ seqpos_slice                 │ (None,)                          │\n",
      "└──────────────────────────────┴──────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(gemma_2_2b_sae.cfg.__dict__.items(), headers=[\"name\", \"value\"], tablefmt=\"simple_outline\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the test dataset\n",
    "\n",
    "The prompts are stored in file `arithmetic_questions.txt` with answers stored in `ans_list` as a `namedtuple`. To generate prompts, use\n",
    "\n",
    "```python\n",
    "q_list, a_list = prompt_generator(with_instructions=False, n_batch=10)\n",
    "# gemma_2_2b.generate(q_list[3], max_new_tokens=10, do_sample=False)\n",
    "qs_tkns = gemma_2_2b.to_tokens(q_list, padding_side=\"left\")\n",
    "ans_tkns = gemma_2_2b.generate(qs_tkns, max_new_tokens=MAX_NEW_TOKENS, do_sample=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NEW_TOKENS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b566f0f599924f27a5083478835ff1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "AnsConfig = namedtuple(\"AnsConfig\", [\"a\", \"b\", \"operation\", \"ans\"])\n",
    "\n",
    "def prompt_generator(\n",
    "    n_range: int = 100,\n",
    "    op: list[str] = [\"plus\"],\n",
    "    n_batch: int = 100,\n",
    "    return_type: Literal[\"string\", \"token\"] = \"string\",\n",
    "    write_to_file: bool = False,\n",
    "    file_path: str = \"addition_prompts.txt\",\n",
    "    with_instructions: bool = False,\n",
    ") -> tuple[list[str], list[AnsConfig]]:\n",
    "    \"\"\"\n",
    "    Generates a list of arithmetic questions and their answers.\n",
    "    \"\"\"\n",
    "    a = t.randint(0, n_range, (n_batch,))\n",
    "    b = t.randint(0, n_range, (n_batch,))\n",
    "    \n",
    "    a_instr = t.randint(0, n_range, (n_batch,))\n",
    "    b_instr = t.randint(0, n_range, (n_batch,))\n",
    "    \n",
    "    ans_list = []\n",
    "    q_list = []\n",
    "    \n",
    "    \n",
    "    with open(file_path, \"w\") as f:\n",
    "        for i in range(n_batch):\n",
    "            \n",
    "            operation = random.choice(op)\n",
    "\n",
    "            # log the correct answer\n",
    "            if operation == \"plus\":\n",
    "                answer = a[i] + b[i]\n",
    "                inst_answer = a_instr[i] + b_instr[i]\n",
    "            elif operation == \"minus\":\n",
    "                answer = a[i] - b[i]\n",
    "                inst_answer = a_instr[i] - b_instr[i]\n",
    "            elif operation == \"times\":\n",
    "                answer = a[i] * b[i]\n",
    "                inst_answer = a_instr[i] * b_instr[i]\n",
    "            # elif operation == \"divided by\":\n",
    "            #     answer = a[i] / b[i]\n",
    "            \n",
    "            if with_instructions:\n",
    "                q_list.append(\n",
    "                    f\"{a_instr[i].item()} {operation} {b_instr[i].item()} is {inst_answer.item()}, {a[i].item()} {operation} {b[i].item()} is\"\n",
    "                )\n",
    "            else:\n",
    "                q_list.append(\n",
    "                    f\"{a[i].item()} {operation} {b[i].item()} is\"\n",
    "                )\n",
    "\n",
    "            if write_to_file:\n",
    "                f.write(q_list[-1] + \"\\n\")\n",
    "            \n",
    "            ans_list.append(\n",
    "                AnsConfig(\n",
    "                    a=a[i].item(),\n",
    "                    b=b[i].item(),\n",
    "                    operation=operation,\n",
    "                    ans=answer.item()\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    return q_list, ans_list\n",
    "\n",
    "### Testing functionality\n",
    "q_list, a_list = prompt_generator(with_instructions=False, n_batch=10)\n",
    "# gemma_2_2b.generate(q_list[3], max_new_tokens=10, do_sample=False)\n",
    "qs_tkns = gemma_2_2b.to_tokens(q_list, padding_side=\"left\")\n",
    "ans_tkns = gemma_2_2b.generate(qs_tkns, max_new_tokens=MAX_NEW_TOKENS, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.00%\n"
     ]
    }
   ],
   "source": [
    "### ans = gemma_2_2b.to_str_tokens(ans_tkns)\n",
    "### check the answer is correct\n",
    "\n",
    "ans = [ans_tkns[i, -MAX_NEW_TOKENS:].tolist() for i in range(ans_tkns.shape[0])]\n",
    "ans = gemma_2_2b.to_string(ans)\n",
    "correct = [\n",
    "    str(a_list[i].ans) in ans[i] for i in range(len(ans))\n",
    "]\n",
    "correct = t.tensor(correct)\n",
    "acc = correct.sum() / correct.shape[0]\n",
    "print(f\"Accuracy: {acc.item():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the accuracy of instructed vs non-instructed prompts:\n",
    "qs_instr, a_list_instr = prompt_generator(with_instructions=True, n_batch=100)\n",
    "qs_no_instr, a_list_no_instr = prompt_generator(with_instructions=False, n_batch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the model's behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT-2-small does not perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"3 plus 5 is 8, 4 plus 6 is\"\n",
    ")\n",
    "gpt2(prompt, return_type=\"logits\", prepend_bos=True)\n",
    "\n",
    "gpt2.generate(\n",
    "    prompt,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"arithmetic_questions.txt\") as f:\n",
    "    questions = f.readlines()\n",
    "    questions = gpt2.to_tokens(questions)\n",
    "    questions = questions[:, :-1]\n",
    "    print(questions.shape)\n",
    "\n",
    "logits: Tensor = gpt2(\n",
    "    questions,\n",
    "    return_type=\"logits\",\n",
    ")\n",
    "\n",
    "answer_logits = logits[:, -1, :].argmax(-1) # prediction of the model\n",
    "answers = gpt2.to_str_tokens(answer_logits)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gemma-2.2b performs with success probability close to 1\n",
    "\n",
    "Observation: the tokenizer mostly treats individual number as individual tokens, and within the prompt there are no words that span multiple tokens.\n",
    "\n",
    "- Comparison: success probability with/without the previous prompt? (97% compared to 87% for 100 uniformly random prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The answer to 9 plus 9 is\"\n",
    "str_ans = gemma_2_2b.generate(prompt, max_new_tokens=100, do_sample=False)\n",
    "print(str_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis with SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find max-activating latents:\n",
    "\n",
    "def get_topk_activated_latents(\n",
    "    sae: SAE = gemma_2_2b_sae,\n",
    "    k: int = 100,\n",
    "    prompts: list[str] = ans_tkns, # use the answers to examine which latents are activated at near the answer tokens\n",
    "    plot_hist: bool = False,\n",
    "):\n",
    "    logits, cache = gemma_2_2b.run_with_cache_with_saes(\n",
    "        prompts,\n",
    "        saes=[sae],\n",
    "        return_type=\"logits\",\n",
    "    )\n",
    "    \n",
    "    latent_acts = cache[\n",
    "        f\"{sae.cfg.hook_name}.hook_sae_acts_pre\"\n",
    "    ][:, -MAX_NEW_TOKENS:, :].mean(1)\n",
    "    \n",
    "    top_activations = latent_acts.topk(k, dim=-1)\n",
    "    \n",
    "    if plot_hist:\n",
    "        px.histogram(\n",
    "            latent_acts.cpu().numpy(),\n",
    "            title=f\"Latent activations at the final token position\",\n",
    "            labels={\"value\": \"Activation\"},\n",
    "            width=800,\n",
    "        ).update_layout(showlegend=False).show()\n",
    "        \n",
    "    \n",
    "    return top_activations\n",
    "\n",
    "top_activations = get_topk_activated_latents(gemma_2_2b_sae, k=6, plot_hist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 most activated latents: [(12132, 9), (9768, 9), (8684, 6), (435, 5), (7900, 4)]\n"
     ]
    }
   ],
   "source": [
    "# fetch the top latents that has been activated\n",
    "from collections import Counter\n",
    "top_act_inds = top_activations.indices.flatten().tolist()\n",
    "occ = Counter(top_act_inds)\n",
    "top_3 = occ.most_common(5)\n",
    "print(\"Top 3 most activated latents:\", top_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other functions\n",
    "\n",
    "...that are not used at current stage but might come in handy later, imported from tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_largest_indices(\n",
    "    x: Float[Tensor, \"batch seq\"],\n",
    "    k: int,\n",
    "    buffer: int = 0,\n",
    "    no_overlap: bool = True,\n",
    ") -> Int[Tensor, \"k 2\"]:\n",
    "    \"\"\"\n",
    "    Returns the tensor of (batch, seqpos) indices for each of the top k elements in the tensor x.\n",
    "\n",
    "    Args:\n",
    "        buffer:     We won't choose any elements within `buffer` from the start or end of their seq (this helps if we\n",
    "                    want more context around the chosen tokens).\n",
    "        no_overlap: If True, this ensures that no 2 top-activating tokens are in the same seq and within `buffer` of\n",
    "                    each other.\n",
    "    \"\"\"\n",
    "    assert buffer * 2 < x.size(1), \"Buffer is too large for the sequence length\"\n",
    "    assert not no_overlap or k <= x.size(0), \"Not enough sequences to have a different token in each sequence\"\n",
    "\n",
    "    if buffer > 0:\n",
    "        x = x[:, buffer:-buffer]\n",
    "\n",
    "    indices = x.flatten().argsort(-1, descending=True)\n",
    "    rows = indices // x.size(1)\n",
    "    cols = indices % x.size(1) + buffer\n",
    "\n",
    "    if no_overlap:\n",
    "        unique_indices = t.empty((0, 2), device=x.device).long()\n",
    "        while len(unique_indices) < k:\n",
    "            unique_indices = t.cat((unique_indices, t.tensor([[rows[0], cols[0]]], device=x.device)))\n",
    "            is_overlapping_mask = (rows == rows[0]) & ((cols - cols[0]).abs() <= buffer)\n",
    "            rows = rows[~is_overlapping_mask]\n",
    "            cols = cols[~is_overlapping_mask]\n",
    "        return unique_indices\n",
    "\n",
    "    return t.stack((rows, cols), dim=1)[:k]\n",
    "\n",
    "def index_with_buffer(\n",
    "    x: Float[Tensor, \"batch seq\"], indices: Int[Tensor, \"k 2\"], buffer: int | None = None\n",
    ") -> Float[Tensor, \"k *buffer_x2_plus1\"]:\n",
    "    \"\"\"\n",
    "    Indexes into `x` with `indices` (which should have come from the `get_k_largest_indices` function), and takes a\n",
    "    +-buffer range around each indexed element. If `indices` are less than `buffer` away from the start of a sequence\n",
    "    then we just take the first `2*buffer+1` elems (same for at the end of a sequence).\n",
    "\n",
    "    If `buffer` is None, then we don't add any buffer and just return the elements at the given indices.\n",
    "    \"\"\"\n",
    "    rows, cols = indices.unbind(dim=-1)\n",
    "    if buffer is not None:\n",
    "        rows = einops.repeat(rows, \"k -> k buffer\", buffer=buffer * 2 + 1)\n",
    "        cols[cols < buffer] = buffer\n",
    "        cols[cols > x.size(1) - buffer - 1] = x.size(1) - buffer - 1\n",
    "        cols = einops.repeat(cols, \"k -> k buffer\", buffer=buffer * 2 + 1) + t.arange(\n",
    "            -buffer, buffer + 1, device=cols.device\n",
    "        )\n",
    "    return x[rows, cols]\n",
    "\n",
    "\n",
    "def show_top_logits(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    latent_idx: int,\n",
    "    k: int = 10,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Displays the top & bottom logits for a particular latent.\n",
    "    \"\"\"\n",
    "    with t.inference_mode():\n",
    "        # in the SAE for gpt2 there's only one instance\n",
    "        latent_vec = sae.W_dec[latent_idx]\n",
    "        token_logit = latent_vec @ model.W_U # adding bias seems to ruin the model...\n",
    "        \n",
    "        token_logit = token_logit.squeeze()\n",
    "        top_k_logits = token_logit.topk(k=k)\n",
    "        bottom_k_logits = token_logit.topk(k=k, largest=False)\n",
    "        \n",
    "        top_k_tokens = model.to_str_tokens(top_k_logits.indices)\n",
    "        bottom_k_tokens = model.to_str_tokens(bottom_k_logits.indices)\n",
    "        \n",
    "        print(f\"Top {k} logits:\")\n",
    "        for i, (logit, token) in enumerate(zip(top_k_logits.values, top_k_tokens)):\n",
    "            print(f\"{i+1}. {token} ({logit:.2f})\")\n",
    "        \n",
    "        print(f\"\\nBottom {k} logits:\")\n",
    "        for i, (logit, token) in enumerate(zip(bottom_k_logits.values, bottom_k_tokens)):\n",
    "            print(f\"{i+1}. {token} ({logit:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_saes = {\n",
    "    layer: SAE.from_pretrained(\n",
    "        \"gpt2-small-hook-z-kk\",\n",
    "        f\"blocks.{layer}.hook_z\",\n",
    "        device=str(device),\n",
    "    )[0]\n",
    "    for layer in range(gpt2.cfg.n_layers)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AttnSeqDFA:\n",
    "    act: float\n",
    "    str_toks_dest: list[str]\n",
    "    str_toks_src: list[str]\n",
    "    dest_pos: int\n",
    "    src_pos: int\n",
    "\n",
    "\n",
    "def display_top_seqs_attn(data: list[AttnSeqDFA]):\n",
    "    \"\"\"\n",
    "    Same as previous function, but we now have 2 str_tok lists and 2 sequence positions to highlight, the first being\n",
    "    for top activations (destination token) and the second for top DFA (src token). We've given you a dataclass to help\n",
    "    keep track of this.\n",
    "    \"\"\"\n",
    "    table = Table(\n",
    "        \"Top Act\",\n",
    "        \"Src token DFA (for top dest token)\",\n",
    "        \"Dest token\",\n",
    "        title=\"Max Activating Examples\",\n",
    "        show_lines=True,\n",
    "    )\n",
    "    for seq in data:\n",
    "        formatted_seqs = [\n",
    "            repr(\n",
    "                \"\".join(\n",
    "                    [f\"[b u {color}]{str_tok}[/]\" if i == seq_pos else str_tok for i, str_tok in enumerate(str_toks)]\n",
    "                )\n",
    "                .replace(\"�\", \"\")\n",
    "                .replace(\"\\n\", \"↵\")\n",
    "            )\n",
    "            for str_toks, seq_pos, color in [\n",
    "                (seq.str_toks_src, seq.src_pos, \"dark_orange\"),\n",
    "                (seq.str_toks_dest, seq.dest_pos, \"green\"),\n",
    "            ]\n",
    "        ]\n",
    "        table.add_row(f\"{seq.act:.3f}\", *formatted_seqs)\n",
    "    rprint(table)\n",
    "\n",
    "\n",
    "str_toks = [\" one\", \" two\", \" three\", \" four\"]\n",
    "example_data = [\n",
    "    AttnSeqDFA(act=0.5, str_toks_dest=str_toks[1:], str_toks_src=str_toks[:-1], dest_pos=0, src_pos=0),\n",
    "    AttnSeqDFA(act=1.5, str_toks_dest=str_toks[1:], str_toks_src=str_toks[:-1], dest_pos=1, src_pos=1),\n",
    "    AttnSeqDFA(act=2.5, str_toks_dest=str_toks[1:], str_toks_src=str_toks[:-1], dest_pos=2, src_pos=0),\n",
    "]\n",
    "display_top_seqs_attn(example_data)\n",
    "\n",
    "\n",
    "\n",
    "_, cache = gpt2.run_with_cache_with_saes(\n",
    "    tokens,\n",
    "    saes=attn_saes[9], # now running with attn_saes\n",
    "    stop_at_layer=attn_saes[9].cfg.hook_layer + 1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_max_activating_examples(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 5,\n",
    "    k: int = 10,\n",
    "    buffer: int = 10,\n",
    ") -> list[tuple[float, list[str], int]]:\n",
    "    \"\"\"\n",
    "    Returns the max activating examples across a number of batches from the activations store.\n",
    "    \"\"\"\n",
    "    list_of_top_values_with_context = []\n",
    "    for i in tqdm(range(total_batches)):\n",
    "        tokens = act_store.get_batch_tokens()\n",
    "        _, cache = model.run_with_cache_with_saes(\n",
    "            tokens,\n",
    "            saes=[sae],\n",
    "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "            names_filter=[f\"{sae.cfg.hook_name}.hook_sae_acts_post\"],\n",
    "        )\n",
    "        acts = cache[f\"{sae.cfg.hook_name}.hook_sae_acts_post\"][..., latent_idx] # for a specific index\n",
    "        # print(acts.shape)\n",
    "        \n",
    "        top_indices = get_k_largest_indices(acts, k=k, buffer=buffer)\n",
    "        top_values_with_context = index_with_buffer(tokens, top_indices, buffer=buffer)\n",
    "        \n",
    "        \n",
    "        list_of_top_values_with_context.extend(\n",
    "            list(\n",
    "                zip(\n",
    "                    [acts[top_indices[i, 0], top_indices[i, 1]] for i in range(k)],\n",
    "                    [model.to_str_tokens(top_values_with_context[i]) for i in range(k)],\n",
    "                    [buffer] * k\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    return sorted(list_of_top_values_with_context, key=lambda x: x[0], reverse=True)[:k]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_max_activating_examples_attn(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    act_store: ActivationsStore,\n",
    "    latent_idx: int,\n",
    "    total_batches: int = 250,\n",
    "    k: int = 10,\n",
    "    buffer: int = 10,\n",
    ") -> list[AttnSeqDFA]:\n",
    "    \"\"\"\n",
    "    Returns the max activating examples across a number of batches from the activations store.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Second attempt\n",
    "    data = []\n",
    "    for i in tqdm(range(total_batches)):\n",
    "        tokens = act_store.get_batch_tokens()\n",
    "        _, cache = model.run_with_cache_with_saes(\n",
    "            tokens,\n",
    "            saes=[sae],\n",
    "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "            names_filter=[\n",
    "                f\"{sae.cfg.hook_name}.hook_sae_acts_pre\",\n",
    "                f\"blocks.{sae.cfg.hook_layer}.attn.hook_v\",\n",
    "                f\"blocks.{sae.cfg.hook_layer}.attn.hook_pattern\",\n",
    "            ],\n",
    "        )\n",
    "        acts = cache[f\"{sae.cfg.hook_name}.hook_sae_acts_pre\"][..., latent_idx]\n",
    "        # shape: (batch, seq)\n",
    "        \n",
    "        # Find the maximally activating tokens with\n",
    "        top_indices = get_k_largest_indices(acts, k=k, buffer=buffer)\n",
    "        top_values_with_context = index_with_buffer(tokens, top_indices, buffer=buffer)\n",
    "        # print(f\"top_values_with_context shape: {top_values_with_context.shape}\")\n",
    "        \n",
    "        # Find the which source tokens are most responsible for the activations\n",
    "        val_vec = cache[f\"blocks.{sae.cfg.hook_layer}.attn.hook_v\"]\n",
    "        # shape (batch, seq, nh, dh)\n",
    "        attn_probs = cache[f\"blocks.{sae.cfg.hook_layer}.attn.hook_pattern\"]\n",
    "        # shape (batch, nh, seq_dest, seq_src)\n",
    "        \n",
    "        # Limit the attention prob and value vectors to the top activations\n",
    "        attn_probs = einops.rearrange(attn_probs, \"batch nh seq_des seq_src -> batch seq_des nh seq_src\")[top_indices[:, 0], top_indices[:, 1]]\n",
    "        # shape (topk, nh, seq_src)\n",
    "        val_vec = val_vec[top_indices[:, 0], :, :, :]\n",
    "        # shape (topk, seq_src, nh, dh)\n",
    "        \n",
    "        val_weighted = einops.einsum(\n",
    "            val_vec, attn_probs,\n",
    "            \"topk seq_src nh dh, topk nh seq_src -> topk seq_src nh dh\"\n",
    "        )\n",
    "        val_weighted = einops.rearrange(val_weighted, \"topk seq_src nh dh -> topk seq_src (nh dh)\")\n",
    "        # or equivalently:\n",
    "        # val_weighted = val_weighted.flatten(-2, -1)\n",
    "        \n",
    "        scores = einops.einsum(\n",
    "            val_weighted, sae.W_enc[:, latent_idx],\n",
    "            \"topk seq_src d_model, d_model -> topk seq_src\"\n",
    "        )\n",
    "        # yet this is restricted to the top activations\n",
    "        # shape (topk, seq_src)\n",
    "        \n",
    "        max_src_inds = scores.argmax(-1)\n",
    "        # shape (topk,), need to reshape this by adding in the batch index\n",
    "        max_src_inds = t.stack((top_indices[:, 0], max_src_inds), dim=1)\n",
    "        \n",
    "        # provide the buffered context around max_src_inds\n",
    "        top_src_with_context = index_with_buffer(tokens, max_src_inds, buffer=buffer)\n",
    "        # shape (batch_, seq_des_, 2*buffer+1)\n",
    "        \n",
    "        ### Finally, log the data\n",
    "        # get the actiavtion values of the scores\n",
    "        top_acts = scores[t.arange(max_src_inds.shape[0]), max_src_inds[:, 1]]\n",
    "        # print(f\"top_acts shape: {top_acts.shape}\")\n",
    "        \n",
    "        # get the str_toks for the top activations\n",
    "        str_toks_dest = [model.to_str_tokens(toks) for toks in top_values_with_context]\n",
    "        str_toks_src = [model.to_str_tokens(toks) for toks in top_src_with_context]\n",
    "        dest_pos = [buffer] * k\n",
    "        src_pos = [buffer] * k\n",
    "        \n",
    "        for act, des_token_w_ctx, src_token_w_ctx, des_index, src_index in zip(\n",
    "            top_acts, str_toks_dest, str_toks_src, dest_pos, src_pos\n",
    "        ):\n",
    "            data.append(\n",
    "                AttnSeqDFA(act=act.item(), str_toks_dest=des_token_w_ctx, str_toks_src=src_token_w_ctx, dest_pos=des_index, src_pos=src_index)\n",
    "            )\n",
    "    \n",
    "    return sorted(data, key=lambda x: x.act, reverse=True)[:k]\n",
    "        \n",
    "        \n",
    "\n",
    "# Test your function: compare it to dashboard above (max DFA should come from src toks like \" guns\", \" firearms\")\n",
    "layer = 9\n",
    "data = fetch_max_activating_examples_attn(gpt2, attn_saes[layer], gpt2_act_store, latent_idx=2)\n",
    "display_top_seqs_attn(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
