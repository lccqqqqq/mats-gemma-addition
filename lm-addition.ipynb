{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM does addition with trignometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/venv_new/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Literal, TypeAlias\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import requests\n",
    "import torch as t\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from IPython.display import HTML, IFrame, clear_output, display\n",
    "from jaxtyping import Float, Int\n",
    "# from openai import OpenAI\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from sae_lens import (\n",
    "    SAE,\n",
    "    ActivationsStore,\n",
    "    HookedSAETransformer,\n",
    "    LanguageModelSAERunnerConfig,\n",
    "    SAEConfig,\n",
    "    SAETrainingRunner,\n",
    "    upload_saes_to_huggingface,\n",
    ")\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "# from sae_vis import SaeVisConfig, SaeVisData, SaeVisLayoutConfig\n",
    "# from tabulate import tabulate\n",
    "from torch import Tensor, nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import ActivationCache, HookedTransformer\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens.utils import get_act_name, test_prompt, to_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "### Outputing tensor shape\n",
    "\n",
    "def s(tensor):\n",
    "    \"\"\"\n",
    "    Simple helper function to print the shape of a tensor.\n",
    "    \n",
    "    Args:\n",
    "        tensor: A PyTorch tensor or any object with a .shape attribute\n",
    "    \n",
    "    Example:\n",
    "        attnout = torch.randn(32, 768)\n",
    "        s(attnout)  # Output: shape of attnout is torch.Size([32, 768])\n",
    "    \"\"\"\n",
    "    # Get the name of the variable from the caller's frame\n",
    "    frame = inspect.currentframe().f_back\n",
    "    calling_line = inspect.getframeinfo(frame).code_context[0].strip()\n",
    "    # Extract variable name from the function call\n",
    "    # This looks for s(variable_name) pattern\n",
    "    import re\n",
    "    match = re.search(r's\\((.*?)\\)', calling_line)\n",
    "    if match:\n",
    "        var_name = match.group(1).strip()\n",
    "    else:\n",
    "        var_name = \"tensor\"\n",
    "        \n",
    "    if hasattr(tensor, 'shape'):\n",
    "        print(f\"Shape of [{var_name}]: {tensor.shape}\")\n",
    "    else:\n",
    "        print(f\"{var_name} has no shape attribute. Type: {type(tensor)}\")\n",
    "        \n",
    "        \n",
    "### Check GPU memory usage\n",
    "def print_gpu_memory():\n",
    "    if t.cuda.is_available():\n",
    "        for i in range(t.cuda.device_count()):\n",
    "            total = t.cuda.get_device_properties(i).total_memory / 1024**3  # Convert to GB\n",
    "            reserved = t.cuda.memory_reserved(i) / 1024**3\n",
    "            allocated = t.cuda.memory_allocated(i) / 1024**3\n",
    "            print(f\"GPU {i}:\")\n",
    "            print(f\"  Total Memory: {total:.2f} GB\")\n",
    "            print(f\"  Reserved Memory: {reserved:.2f} GB\")\n",
    "            print(f\"  Allocated Memory: {allocated:.2f} GB\")\n",
    "            print(f\"  Free Memory: {total - reserved:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model\n",
    "\n",
    "```python\n",
    "gemma = \"gemma-2-2b\"\n",
    "gemma_saes = \"gemma_saes\"\n",
    "gpt = \"gpt-j-6B\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 94.21it/s]\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:14<00:00,  1.81it/s]\n"
     ]
    }
   ],
   "source": [
    "LAYER = 20\n",
    "device = \"cuda\"\n",
    "gemma = HookedSAETransformer.from_pretrained(\"gemma-2-2b\", device=device)\n",
    "gemma_saes = [\n",
    "    SAE.from_pretrained(\n",
    "        \"gemma-scope-2b-pt-res-canonical\",\n",
    "        f\"layer_{i}/width_16k/canonical\",\n",
    "        device=str(device)\n",
    "    )[0]\n",
    "    for i in tqdm(range(gemma.cfg.n_layers))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EleutherAI/gpt-j-6B were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 47.40 GiB of which 171.69 MiB is free. Process 1431206 has 47.22 GiB memory in use. Of the allocated memory 46.13 GiB is allocated by PyTorch, and 608.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gpt \u001b[38;5;241m=\u001b[39m \u001b[43mHookedTransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained_no_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-j-6B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/venv_new/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:1406\u001b[0m, in \u001b[0;36mHookedTransformer.from_pretrained_no_processing\u001b[0;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, fold_value_biases, dtype, default_prepend_bos, default_padding_side, **from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_pretrained_no_processing\u001b[39m(\n\u001b[1;32m   1389\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1399\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfrom_pretrained_kwargs,\n\u001b[1;32m   1400\u001b[0m ):\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for from_pretrained.\u001b[39;00m\n\u001b[1;32m   1402\u001b[0m \n\u001b[1;32m   1403\u001b[0m \u001b[38;5;124;03m    Wrapper for from_pretrained with all boolean flags related to simplifying the model set to\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;124;03m    False. Refer to from_pretrained for details.\u001b[39;00m\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfold_ln\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_ln\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcenter_writing_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter_writing_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcenter_unembed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter_unembed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfold_value_biases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_value_biases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrefactor_factored_attn_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefactor_factored_attn_matrices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_prepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_prepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_padding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_padding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1416\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfrom_pretrained_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1417\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/venv_new/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:1381\u001b[0m, in \u001b[0;36mHookedTransformer.from_pretrained\u001b[0;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, tokenizer, move_to_device, fold_value_biases, default_prepend_bos, default_padding_side, dtype, first_n_layers, **from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m   1371\u001b[0m model\u001b[38;5;241m.\u001b[39mload_and_process_state_dict(\n\u001b[1;32m   1372\u001b[0m     state_dict,\n\u001b[1;32m   1373\u001b[0m     fold_ln\u001b[38;5;241m=\u001b[39mfold_ln,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1377\u001b[0m     refactor_factored_attn_matrices\u001b[38;5;241m=\u001b[39mrefactor_factored_attn_matrices,\n\u001b[1;32m   1378\u001b[0m )\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m move_to_device:\n\u001b[0;32m-> 1381\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove_model_modules_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded pretrained model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m into HookedTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/workspace/venv_new/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:1105\u001b[0m, in \u001b[0;36mHookedTransformer.move_model_modules_to_device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munembed\u001b[38;5;241m.\u001b[39mto(devices\u001b[38;5;241m.\u001b[39mget_best_available_device(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg))\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[0;32m-> 1105\u001b[0m     \u001b[43mblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_available_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/venv_new/lib/python3.10/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/venv_new/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/venv_new/lib/python3.10/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/venv_new/lib/python3.10/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 47.40 GiB of which 171.69 MiB is free. Process 1431206 has 47.22 GiB memory in use. Of the allocated memory 46.13 GiB is allocated by PyTorch, and 608.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "gpt = HookedTransformer.from_pretrained_no_processing(\"gpt-j-6B\", device=\"cuda\", dtype=t.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for generating prompts\n",
    "\n",
    "1. Natural language prompts: *a plus b is*\n",
    "2. Symbol prompts: *a + b =*\n",
    "3. Natural language prompts with instructions: *a plus b is c, d plus e is*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "AnsConfig = namedtuple(\"AnsConfig\", [\"a\", \"b\", \"operation\", \"ans\"])\n",
    "\n",
    "def prompt_generator(\n",
    "    n_range: int = 100,\n",
    "    op: list[str] = [\"plus\"],\n",
    "    n_batch: int = 100,\n",
    "    return_type: Literal[\"string\", \"token\"] = \"string\",\n",
    "    write_to_file: bool = False,\n",
    "    file_path: str = \"addition_prompts.txt\",\n",
    "    with_instructions: bool = False,\n",
    "    with_symbols: bool = False,\n",
    ") -> tuple[list[str], list[AnsConfig]]:\n",
    "    \"\"\"Generates a list of arithmetic questions and their answers.\n",
    "    \"\"\"\n",
    "    a = t.randint(0, n_range, (n_batch,))\n",
    "    b = t.randint(0, n_range, (n_batch,))\n",
    "    \n",
    "    a_instr = t.randint(0, n_range, (n_batch,))\n",
    "    b_instr = t.randint(0, n_range, (n_batch,))\n",
    "    \n",
    "    ans_list = []\n",
    "    q_list = []\n",
    "    \n",
    "    \n",
    "    with open(file_path, \"w\") as f:\n",
    "        for i in range(n_batch):\n",
    "            \n",
    "            \n",
    "            operation = random.choice(op)\n",
    "\n",
    "            if with_symbols:\n",
    "                equal_str = \"=\"\n",
    "                if operation == \"plus\":\n",
    "                    operation_str = \"+\"\n",
    "                elif operation == \"minus\":\n",
    "                    operation_str = \"-\"\n",
    "                elif operation == \"times\":\n",
    "                    operation_str = \"*\"\n",
    "                elif operation == \"divided by\":\n",
    "                    operation_str = \"/\"\n",
    "                else:\n",
    "                    raise ValueError(\"Operation type not recognized\")\n",
    "            else:\n",
    "                equal_str = \"is\"\n",
    "\n",
    "            # log the correct answer\n",
    "            if operation == \"plus\":\n",
    "                answer = a[i] + b[i]\n",
    "                inst_answer = a_instr[i] + b_instr[i]\n",
    "            elif operation == \"minus\":\n",
    "                answer = a[i] - b[i]\n",
    "                inst_answer = a_instr[i] - b_instr[i]\n",
    "            elif operation == \"times\":\n",
    "                answer = a[i] * b[i]\n",
    "                inst_answer = a_instr[i] * b_instr[i]\n",
    "            # elif operation == \"divided by\":\n",
    "            #     answer = a[i] / b[i]\n",
    "            \n",
    "            if with_instructions:\n",
    "                q_list.append(\n",
    "                    f\"{a_instr[i].item()} {operation_str if with_symbols else operation} {b_instr[i].item()} {equal_str} {inst_answer.item()}, {a[i].item()} {operation_str if with_symbols else operation} {b[i].item()} {equal_str}\"\n",
    "                )\n",
    "            else:\n",
    "                q_list.append(\n",
    "                    f\"{a[i].item()} {operation_str if with_symbols else operation} {b[i].item()} {equal_str}\"\n",
    "                )\n",
    "\n",
    "            if write_to_file:\n",
    "                f.write(q_list[-1] + \"\\n\")\n",
    "            \n",
    "            ans_list.append(\n",
    "                AnsConfig(\n",
    "                    a=a[i].item(),\n",
    "                    b=b[i].item(),\n",
    "                    operation=operation,\n",
    "                    ans=answer.item()\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    return q_list, ans_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 29.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>',\n",
       " '29',\n",
       " ' +',\n",
       " ' 21',\n",
       " ' =',\n",
       " ' -',\n",
       " '2',\n",
       " '*',\n",
       " 'z',\n",
       " '.',\n",
       " ' Let',\n",
       " ' y',\n",
       " ' =',\n",
       " ' z',\n",
       " ' -']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Individual example using gpt-j\n",
    "question_tokens = gpt.to_tokens(\n",
    "    \"29 + 21 =\"\n",
    ")\n",
    "max_new_tokens = 10\n",
    "ans_tokens = gpt.generate(\n",
    "    question_tokens,\n",
    "    max_new_tokens = max_new_tokens,\n",
    "    do_sample=False\n",
    ")\n",
    "gpt.to_str_tokens(ans_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 43\\n\\n', ' 149', ' 92\\n\\n', ' 97\\n\\n', '\\n\\n10 +', ' 73\\n\\n', ' 51 +', ' 92\\n\\n', ' 81\\n\\n', ' 38\\n\\n', ' 119', ' 96\\n\\n', ' 119', ' 77 +', ' 105', '\\n\\n67 +', ' 128', ' 128', '\\n\\n9 + ', ' 77\\n\\n', ' 163', ' 110', ' 143', ' 61\\n\\n', ' 129', ' 168', ' 137', ' 146', ' 66\\n\\n', ' 117', ' 96\\n\\n', ' 66\\n\\n', ' 104', '.\\n\\n10', ' 106', ' 126', ' 54\\n\\n', ' 44\\n\\n', ' 72\\n\\n', ' 96\\n\\n', ' 28\\n\\n', ' 155', ' 149', ' 140', ' 65\\n\\n', ' 93\\n\\n', '\\n\\n35 +', ' 76 +', ' 126', ' 121', '\\n\\n65 +', ' 151', ' 146', ' 173', ' 81\\n\\n', ' 103', ' 166', ' 101', ' 159', ' 94\\n\\n', ' 138', ' 113', ' 118', ' 101', ' 92 +', ' 140', ' 71\\n\\n', ' 67\\n\\n', '\\n\\n98 +', '.\\n\\n0 +', ' 116', ' 141', ' 91\\n\\n', ' 88\\n\\n', ' 19 +', ' 121', ' 44\\n\\n', ' 32\\n\\n', ' 120', ' 87\\n\\n', ' 110', ' 76\\n\\n', ' 124', ' 29 +', ' 157', ' 148', ' 100', ' 118', ' 111', ' 116', ' 89\\n\\n', ' 111', ' 49\\n\\n', ' 150', ' 45\\n\\n', ' 85\\n\\n', ' 51\\n\\n', ' 96 +', ' 54\\n\\n', ' 53\\n\\n']\n",
      "Accuracy of gemma-2-2b: 85.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' -', ' -', ' -', ' -', ' -', ' -', ' -', ' 0', ' -', ' -', ' -', ' 0', ' -', ' -', ' 0', ' 0', ' 0', ' -', ' -', ' 0', ' -', ' -', ' 0', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' 0', ' -', ' -', ' 0', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' 0', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' 0', ' -', ' -', ' 0', ' -', ' -', ' -', ' -', ' -', ' -', ' 0', ' 0', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -', ' -']\n",
      "Accuracy of gpt-j-6b: 0.00%\n"
     ]
    }
   ],
   "source": [
    "t.cuda.empty_cache()\n",
    "q_list, a_list = prompt_generator(with_instructions=False, n_batch=100, write_to_file=True, with_symbols=True)\n",
    "\n",
    "models = [gemma, gpt]\n",
    "models_str = [\"gemma-2-2b\", \"gpt-j-6b\"]\n",
    "max_new_tokens_list = [4, 1]\n",
    "\n",
    "for model_str, model, max_new_tokens in zip(models_str, models, max_new_tokens_list):\n",
    "    question_tokens = model.to_tokens(q_list)\n",
    "    answer_tokens = model.generate(\n",
    "        question_tokens,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    \n",
    "    # process the answers\n",
    "    ans = [\n",
    "        answer_tokens[i, -max_new_tokens:].tolist() for i in range(answer_tokens.shape[0])\n",
    "    ]\n",
    "    ans = model.to_string(ans)\n",
    "    print(ans)\n",
    "    correct = [\n",
    "        str(a_list[i].ans) in ans[i] for i in range(len(ans))\n",
    "    ]\n",
    "    correct = t.tensor(correct)\n",
    "    acc = correct.sum() / correct.shape[0]\n",
    "    print(f\"Accuracy of {model_str}: {acc.item():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0:\n",
      "  Total Memory: 47.40 GB\n",
      "  Reserved Memory: 38.07 GB\n",
      "  Allocated Memory: 37.29 GB\n",
      "  Free Memory: 9.33 GB\n",
      "508\n",
      "GPU 0:\n",
      "  Total Memory: 47.40 GB\n",
      "  Reserved Memory: 38.07 GB\n",
      "  Allocated Memory: 37.29 GB\n",
      "  Free Memory: 9.33 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_gpu_memory()\n",
    "print(gc.collect())\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number representations\n",
    "\n",
    "We examine how the number tokens are represented in the 2.2b model. In larger models, it has been found that the all integers from the interval $[0, 360]$ is encoded as a single digit. We\n",
    "\n",
    "1. For smaller models like 2.2b, the numbers are encoded *digit by digit*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma tokens: [['0'], ['1'], ['2'], ['3'], ['4'], ['5'], ['6'], ['7'], ['8'], ['9'], ['1', '0'], ['1', '1'], ['1', '2'], ['1', '3'], ['1', '4'], ['1', '5'], ['1', '6'], ['1', '7'], ['1', '8'], ['1', '9'], ['2', '0'], ['2', '1'], ['2', '2'], ['2', '3'], ['2', '4'], ['2', '5'], ['2', '6'], ['2', '7'], ['2', '8'], ['2', '9'], ['3', '0'], ['3', '1'], ['3', '2'], ['3', '3'], ['3', '4'], ['3', '5'], ['3', '6'], ['3', '7'], ['3', '8'], ['3', '9'], ['4', '0'], ['4', '1'], ['4', '2'], ['4', '3'], ['4', '4'], ['4', '5'], ['4', '6'], ['4', '7'], ['4', '8'], ['4', '9'], ['5', '0'], ['5', '1'], ['5', '2'], ['5', '3'], ['5', '4'], ['5', '5'], ['5', '6'], ['5', '7'], ['5', '8'], ['5', '9'], ['6', '0'], ['6', '1'], ['6', '2'], ['6', '3'], ['6', '4'], ['6', '5'], ['6', '6'], ['6', '7'], ['6', '8'], ['6', '9'], ['7', '0'], ['7', '1'], ['7', '2'], ['7', '3'], ['7', '4'], ['7', '5'], ['7', '6'], ['7', '7'], ['7', '8'], ['7', '9'], ['8', '0'], ['8', '1'], ['8', '2'], ['8', '3'], ['8', '4'], ['8', '5'], ['8', '6'], ['8', '7'], ['8', '8'], ['8', '9'], ['9', '0'], ['9', '1'], ['9', '2'], ['9', '3'], ['9', '4'], ['9', '5'], ['9', '6'], ['9', '7'], ['9', '8'], ['9', '9'], ['1', '0', '0'], ['1', '0', '1'], ['1', '0', '2'], ['1', '0', '3'], ['1', '0', '4'], ['1', '0', '5'], ['1', '0', '6'], ['1', '0', '7'], ['1', '0', '8'], ['1', '0', '9'], ['1', '1', '0'], ['1', '1', '1'], ['1', '1', '2'], ['1', '1', '3'], ['1', '1', '4'], ['1', '1', '5'], ['1', '1', '6'], ['1', '1', '7'], ['1', '1', '8'], ['1', '1', '9'], ['1', '2', '0'], ['1', '2', '1'], ['1', '2', '2'], ['1', '2', '3'], ['1', '2', '4'], ['1', '2', '5'], ['1', '2', '6'], ['1', '2', '7'], ['1', '2', '8'], ['1', '2', '9'], ['1', '3', '0'], ['1', '3', '1'], ['1', '3', '2'], ['1', '3', '3'], ['1', '3', '4'], ['1', '3', '5'], ['1', '3', '6'], ['1', '3', '7'], ['1', '3', '8'], ['1', '3', '9'], ['1', '4', '0'], ['1', '4', '1'], ['1', '4', '2'], ['1', '4', '3'], ['1', '4', '4'], ['1', '4', '5'], ['1', '4', '6'], ['1', '4', '7'], ['1', '4', '8'], ['1', '4', '9'], ['1', '5', '0'], ['1', '5', '1'], ['1', '5', '2'], ['1', '5', '3'], ['1', '5', '4'], ['1', '5', '5'], ['1', '5', '6'], ['1', '5', '7'], ['1', '5', '8'], ['1', '5', '9'], ['1', '6', '0'], ['1', '6', '1'], ['1', '6', '2'], ['1', '6', '3'], ['1', '6', '4'], ['1', '6', '5'], ['1', '6', '6'], ['1', '6', '7'], ['1', '6', '8'], ['1', '6', '9'], ['1', '7', '0'], ['1', '7', '1'], ['1', '7', '2'], ['1', '7', '3'], ['1', '7', '4'], ['1', '7', '5'], ['1', '7', '6'], ['1', '7', '7'], ['1', '7', '8'], ['1', '7', '9'], ['1', '8', '0'], ['1', '8', '1'], ['1', '8', '2'], ['1', '8', '3'], ['1', '8', '4'], ['1', '8', '5'], ['1', '8', '6'], ['1', '8', '7'], ['1', '8', '8'], ['1', '8', '9'], ['1', '9', '0'], ['1', '9', '1'], ['1', '9', '2'], ['1', '9', '3'], ['1', '9', '4'], ['1', '9', '5'], ['1', '9', '6'], ['1', '9', '7'], ['1', '9', '8'], ['1', '9', '9'], ['2', '0', '0'], ['2', '0', '1'], ['2', '0', '2'], ['2', '0', '3'], ['2', '0', '4'], ['2', '0', '5'], ['2', '0', '6'], ['2', '0', '7'], ['2', '0', '8'], ['2', '0', '9'], ['2', '1', '0'], ['2', '1', '1'], ['2', '1', '2'], ['2', '1', '3'], ['2', '1', '4'], ['2', '1', '5'], ['2', '1', '6'], ['2', '1', '7'], ['2', '1', '8'], ['2', '1', '9'], ['2', '2', '0'], ['2', '2', '1'], ['2', '2', '2'], ['2', '2', '3'], ['2', '2', '4'], ['2', '2', '5'], ['2', '2', '6'], ['2', '2', '7'], ['2', '2', '8'], ['2', '2', '9'], ['2', '3', '0'], ['2', '3', '1'], ['2', '3', '2'], ['2', '3', '3'], ['2', '3', '4'], ['2', '3', '5'], ['2', '3', '6'], ['2', '3', '7'], ['2', '3', '8'], ['2', '3', '9'], ['2', '4', '0'], ['2', '4', '1'], ['2', '4', '2'], ['2', '4', '3'], ['2', '4', '4'], ['2', '4', '5'], ['2', '4', '6'], ['2', '4', '7'], ['2', '4', '8'], ['2', '4', '9'], ['2', '5', '0'], ['2', '5', '1'], ['2', '5', '2'], ['2', '5', '3'], ['2', '5', '4'], ['2', '5', '5'], ['2', '5', '6'], ['2', '5', '7'], ['2', '5', '8'], ['2', '5', '9'], ['2', '6', '0'], ['2', '6', '1'], ['2', '6', '2'], ['2', '6', '3'], ['2', '6', '4'], ['2', '6', '5'], ['2', '6', '6'], ['2', '6', '7'], ['2', '6', '8'], ['2', '6', '9'], ['2', '7', '0'], ['2', '7', '1'], ['2', '7', '2'], ['2', '7', '3'], ['2', '7', '4'], ['2', '7', '5'], ['2', '7', '6'], ['2', '7', '7'], ['2', '7', '8'], ['2', '7', '9'], ['2', '8', '0'], ['2', '8', '1'], ['2', '8', '2'], ['2', '8', '3'], ['2', '8', '4'], ['2', '8', '5'], ['2', '8', '6'], ['2', '8', '7'], ['2', '8', '8'], ['2', '8', '9'], ['2', '9', '0'], ['2', '9', '1'], ['2', '9', '2'], ['2', '9', '3'], ['2', '9', '4'], ['2', '9', '5'], ['2', '9', '6'], ['2', '9', '7'], ['2', '9', '8'], ['2', '9', '9'], ['3', '0', '0'], ['3', '0', '1'], ['3', '0', '2'], ['3', '0', '3'], ['3', '0', '4'], ['3', '0', '5'], ['3', '0', '6'], ['3', '0', '7'], ['3', '0', '8'], ['3', '0', '9'], ['3', '1', '0'], ['3', '1', '1'], ['3', '1', '2'], ['3', '1', '3'], ['3', '1', '4'], ['3', '1', '5'], ['3', '1', '6'], ['3', '1', '7'], ['3', '1', '8'], ['3', '1', '9'], ['3', '2', '0'], ['3', '2', '1'], ['3', '2', '2'], ['3', '2', '3'], ['3', '2', '4'], ['3', '2', '5'], ['3', '2', '6'], ['3', '2', '7'], ['3', '2', '8'], ['3', '2', '9'], ['3', '3', '0'], ['3', '3', '1'], ['3', '3', '2'], ['3', '3', '3'], ['3', '3', '4'], ['3', '3', '5'], ['3', '3', '6'], ['3', '3', '7'], ['3', '3', '8'], ['3', '3', '9'], ['3', '4', '0'], ['3', '4', '1'], ['3', '4', '2'], ['3', '4', '3'], ['3', '4', '4'], ['3', '4', '5'], ['3', '4', '6'], ['3', '4', '7'], ['3', '4', '8'], ['3', '4', '9'], ['3', '5', '0'], ['3', '5', '1'], ['3', '5', '2'], ['3', '5', '3'], ['3', '5', '4'], ['3', '5', '5'], ['3', '5', '6'], ['3', '5', '7'], ['3', '5', '8'], ['3', '5', '9'], ['3', '6', '0'], ['3', '6', '1'], ['3', '6', '2'], ['3', '6', '3'], ['3', '6', '4'], ['3', '6', '5'], ['3', '6', '6'], ['3', '6', '7'], ['3', '6', '8'], ['3', '6', '9'], ['3', '7', '0'], ['3', '7', '1'], ['3', '7', '2'], ['3', '7', '3'], ['3', '7', '4'], ['3', '7', '5'], ['3', '7', '6'], ['3', '7', '7'], ['3', '7', '8'], ['3', '7', '9'], ['3', '8', '0'], ['3', '8', '1'], ['3', '8', '2'], ['3', '8', '3'], ['3', '8', '4'], ['3', '8', '5'], ['3', '8', '6'], ['3', '8', '7'], ['3', '8', '8'], ['3', '8', '9'], ['3', '9', '0'], ['3', '9', '1'], ['3', '9', '2'], ['3', '9', '3'], ['3', '9', '4'], ['3', '9', '5'], ['3', '9', '6'], ['3', '9', '7'], ['3', '9', '8'], ['3', '9', '9'], ['4', '0', '0'], ['4', '0', '1'], ['4', '0', '2'], ['4', '0', '3'], ['4', '0', '4'], ['4', '0', '5'], ['4', '0', '6'], ['4', '0', '7'], ['4', '0', '8'], ['4', '0', '9'], ['4', '1', '0'], ['4', '1', '1'], ['4', '1', '2'], ['4', '1', '3'], ['4', '1', '4'], ['4', '1', '5'], ['4', '1', '6'], ['4', '1', '7'], ['4', '1', '8'], ['4', '1', '9'], ['4', '2', '0'], ['4', '2', '1'], ['4', '2', '2'], ['4', '2', '3'], ['4', '2', '4'], ['4', '2', '5'], ['4', '2', '6'], ['4', '2', '7'], ['4', '2', '8'], ['4', '2', '9'], ['4', '3', '0'], ['4', '3', '1'], ['4', '3', '2'], ['4', '3', '3'], ['4', '3', '4'], ['4', '3', '5'], ['4', '3', '6'], ['4', '3', '7'], ['4', '3', '8'], ['4', '3', '9'], ['4', '4', '0'], ['4', '4', '1'], ['4', '4', '2'], ['4', '4', '3'], ['4', '4', '4'], ['4', '4', '5'], ['4', '4', '6'], ['4', '4', '7'], ['4', '4', '8'], ['4', '4', '9'], ['4', '5', '0'], ['4', '5', '1'], ['4', '5', '2'], ['4', '5', '3'], ['4', '5', '4'], ['4', '5', '5'], ['4', '5', '6'], ['4', '5', '7'], ['4', '5', '8'], ['4', '5', '9'], ['4', '6', '0'], ['4', '6', '1'], ['4', '6', '2'], ['4', '6', '3'], ['4', '6', '4'], ['4', '6', '5'], ['4', '6', '6'], ['4', '6', '7'], ['4', '6', '8'], ['4', '6', '9'], ['4', '7', '0'], ['4', '7', '1'], ['4', '7', '2'], ['4', '7', '3'], ['4', '7', '4'], ['4', '7', '5'], ['4', '7', '6'], ['4', '7', '7'], ['4', '7', '8'], ['4', '7', '9'], ['4', '8', '0'], ['4', '8', '1'], ['4', '8', '2'], ['4', '8', '3'], ['4', '8', '4'], ['4', '8', '5'], ['4', '8', '6'], ['4', '8', '7'], ['4', '8', '8'], ['4', '8', '9'], ['4', '9', '0'], ['4', '9', '1'], ['4', '9', '2'], ['4', '9', '3'], ['4', '9', '4'], ['4', '9', '5'], ['4', '9', '6'], ['4', '9', '7'], ['4', '9', '8'], ['4', '9', '9']]\n",
      "GPT-J tokens: [['0'], ['1'], ['2'], ['3'], ['4'], ['5'], ['6'], ['7'], ['8'], ['9'], ['10'], ['11'], ['12'], ['13'], ['14'], ['15'], ['16'], ['17'], ['18'], ['19'], ['20'], ['21'], ['22'], ['23'], ['24'], ['25'], ['26'], ['27'], ['28'], ['29'], ['30'], ['31'], ['32'], ['33'], ['34'], ['35'], ['36'], ['37'], ['38'], ['39'], ['40'], ['41'], ['42'], ['43'], ['44'], ['45'], ['46'], ['47'], ['48'], ['49'], ['50'], ['51'], ['52'], ['53'], ['54'], ['55'], ['56'], ['57'], ['58'], ['59'], ['60'], ['61'], ['62'], ['63'], ['64'], ['65'], ['66'], ['67'], ['68'], ['69'], ['70'], ['71'], ['72'], ['73'], ['74'], ['75'], ['76'], ['77'], ['78'], ['79'], ['80'], ['81'], ['82'], ['83'], ['84'], ['85'], ['86'], ['87'], ['88'], ['89'], ['90'], ['91'], ['92'], ['93'], ['94'], ['95'], ['96'], ['97'], ['98'], ['99'], ['100'], ['101'], ['102'], ['103'], ['104'], ['105'], ['106'], ['107'], ['108'], ['109'], ['110'], ['111'], ['112'], ['113'], ['114'], ['115'], ['116'], ['117'], ['118'], ['119'], ['120'], ['121'], ['122'], ['123'], ['124'], ['125'], ['126'], ['127'], ['128'], ['129'], ['130'], ['131'], ['132'], ['133'], ['134'], ['135'], ['136'], ['137'], ['138'], ['139'], ['140'], ['141'], ['142'], ['143'], ['144'], ['145'], ['146'], ['147'], ['148'], ['149'], ['150'], ['151'], ['152'], ['153'], ['154'], ['155'], ['156'], ['157'], ['158'], ['159'], ['160'], ['161'], ['162'], ['163'], ['164'], ['165'], ['166'], ['167'], ['168'], ['169'], ['170'], ['171'], ['172'], ['173'], ['174'], ['175'], ['176'], ['177'], ['178'], ['179'], ['180'], ['181'], ['182'], ['183'], ['184'], ['185'], ['186'], ['187'], ['188'], ['189'], ['190'], ['191'], ['192'], ['193'], ['194'], ['195'], ['196'], ['197'], ['198'], ['199'], ['200'], ['201'], ['202'], ['203'], ['204'], ['205'], ['206'], ['207'], ['208'], ['209'], ['210'], ['211'], ['212'], ['213'], ['214'], ['215'], ['216'], ['217'], ['218'], ['219'], ['220'], ['221'], ['222'], ['223'], ['224'], ['225'], ['226'], ['227'], ['228'], ['229'], ['230'], ['231'], ['232'], ['233'], ['234'], ['235'], ['236'], ['237'], ['238'], ['239'], ['240'], ['241'], ['242'], ['243'], ['244'], ['245'], ['246'], ['247'], ['248'], ['249'], ['250'], ['251'], ['252'], ['253'], ['254'], ['255'], ['256'], ['257'], ['258'], ['259'], ['260'], ['261'], ['262'], ['263'], ['264'], ['265'], ['266'], ['267'], ['268'], ['269'], ['270'], ['271'], ['272'], ['273'], ['274'], ['275'], ['276'], ['277'], ['278'], ['279'], ['280'], ['281'], ['282'], ['283'], ['284'], ['285'], ['286'], ['287'], ['288'], ['289'], ['290'], ['291'], ['292'], ['293'], ['294'], ['295'], ['296'], ['297'], ['298'], ['299'], ['300'], ['301'], ['302'], ['303'], ['304'], ['305'], ['306'], ['307'], ['308'], ['309'], ['310'], ['311'], ['312'], ['313'], ['314'], ['315'], ['316'], ['317'], ['318'], ['319'], ['320'], ['321'], ['322'], ['323'], ['324'], ['325'], ['326'], ['327'], ['328'], ['329'], ['330'], ['331'], ['332'], ['333'], ['334'], ['335'], ['336'], ['337'], ['338'], ['339'], ['340'], ['341'], ['342'], ['343'], ['344'], ['345'], ['346'], ['347'], ['348'], ['349'], ['350'], ['351'], ['352'], ['353'], ['354'], ['355'], ['356'], ['357'], ['358'], ['359'], ['360'], ['361'], ['362'], ['363'], ['364'], ['365'], ['366'], ['367'], ['368'], ['369'], ['370'], ['371'], ['372'], ['373'], ['374'], ['375'], ['376'], ['377'], ['378'], ['379'], ['380'], ['381'], ['382'], ['383'], ['384'], ['385'], ['386'], ['387'], ['388'], ['389'], ['390'], ['391'], ['392'], ['393'], ['394'], ['395'], ['396'], ['397'], ['398'], ['399'], ['400'], ['401'], ['402'], ['403'], ['404'], ['405'], ['406'], ['407'], ['408'], ['409'], ['410'], ['411'], ['412'], ['413'], ['414'], ['415'], ['416'], ['417'], ['418'], ['419'], ['420'], ['421'], ['422'], ['423'], ['424'], ['425'], ['426'], ['427'], ['428'], ['429'], ['430'], ['431'], ['432'], ['433'], ['434'], ['435'], ['436'], ['437'], ['438'], ['439'], ['440'], ['441'], ['442'], ['443'], ['444'], ['445'], ['446'], ['447'], ['448'], ['449'], ['450'], ['451'], ['452'], ['453'], ['454'], ['455'], ['456'], ['457'], ['458'], ['459'], ['460'], ['461'], ['462'], ['463'], ['464'], ['465'], ['466'], ['467'], ['468'], ['469'], ['470'], ['471'], ['472'], ['473'], ['474'], ['475'], ['476'], ['477'], ['478'], ['479'], ['480'], ['481'], ['482'], ['483'], ['484'], ['485'], ['486'], ['487'], ['488'], ['489'], ['490'], ['491'], ['492'], ['493'], ['494'], ['495'], ['496'], ['497'], ['498'], ['499']]\n"
     ]
    }
   ],
   "source": [
    "## numbers encoded digit by digit\n",
    "str_nums = [str(i) for i in range(500)]\n",
    "tokens = gemma.to_str_tokens(str_nums, prepend_bos=False)\n",
    "tokens_gpt = gpt.to_str_tokens(str_nums, prepend_bos=False)\n",
    "print(\"Gemma tokens:\", tokens)\n",
    "print(\"GPT-J tokens:\", tokens_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
